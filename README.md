# Radio Murakami

**Welcome to the Radio Murakami repository!**

Here, we develop a bot that spouts out quotes as good as the writings of [Haruki Murakami](https://en.wikipedia.org/wiki/Haruki_Murakami) himself. This is based off his writing style as well as quotes and interviews he has done in the past.

## Data


## Model

We fine tuned a GPT-2 and BERT model using the datasets above.

## References

[1] Radford A., Wu J., Child R., Luan D., Amodei D., and Sutskever I., "Language Models are Unsupervised Multitask Learners", 2019. ([link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))

[2] Devlin J., Chang M-W., Lee K., and Toutanova K., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume I (Long and Short Papers), pp. 4171-4186, June 2019. ([link](https://www.aclweb.org/anthology/N19-1423/))

[3] Vaswani A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I, "Attention is all you need", Advances in Neural Information Processing Systems, pp. 5998–6008, 2017. ([link](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf))
