# Radio Murakami

**Welcome to the Radio Murakami repository!**

## Data

## Model


## References

[1] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, "Language Models are Unsupervised Multitask Learners", 2019. [link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

[2] J. Devlin, M-W Chang, K. Lee, K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume I (Long and Short Papers), June 2019, pp. 4171-4186. [link](https://www.aclweb.org/anthology/N19-1423/)
